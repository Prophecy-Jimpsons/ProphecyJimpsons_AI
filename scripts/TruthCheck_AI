import torch
from transformers import AutoModelForCausalLM, AutoProcessor

def load_aria_model():
    model_id = "rhymes-ai/Aria"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    return model, processor

def verify_event(model, processor, event_text):
    messages = [{
        "role": "user",
        "content": [
            {
                "text": f"Verify if this event is true or false: {event_text}",
                "type": "text"
            }
        ]
    }]
    
    # Process input
    text = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=text, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Generate response
    with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.bfloat16):
        output = model.generate(
            **inputs,
            max_new_tokens=200,
            stop_strings=["<|im_end|>"],
            tokenizer=processor.tokenizer,
            do_sample=True,
            temperature=0.7
        )
        output_ids = output[0][inputs["input_ids"].shape[1]:]
        result = processor.decode(output_ids, skip_special_tokens=True)
        
        # Simple classification based on response
        result = result.lower()
        if any(word in result for word in ['true', 'correct', 'verified', 'confirmed']):
            return "APPROVED", result
        elif any(word in result for word in ['false', 'incorrect', 'fake', 'unverified']):
            return "REJECTED", result
        else:
            return "UNCERTAIN", result

def main():
    # Initialize model
    print("Loading model...")
    model, processor = load_aria_model()
    
    while True:
        event = input("\nEnter a world event to verify (or 'quit' to exit): ")
        if event.lower() == 'quit':
            break
            
        print("\nVerifying event...")
        status, explanation = verify_event(model, processor, event)
        print(f"\nStatus: {status}")
        print(f"Explanation: {explanation}")

if __name__ == "__main__":
    main()
